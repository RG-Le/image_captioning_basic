# image_captioning_basic

This repository contains an Image-Captioning model that uses Flicker8k dataset, to caption images. The archtitection is a Vision-Language one i.e CNN-LSTM. We have used VGG16 to extract features from the images and then mapped each images to possible captions, and then trained the Vision-Language model to predict best words that will deescribe the given images.

The current model uses VGG16, but we can use EfficientNet or anyother model. Also using Attention mechanism in language and vision tasks can improve the accuracy very efficiently.

Will do that later....
